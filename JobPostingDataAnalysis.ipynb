{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f897af0a",
   "metadata": {},
   "source": [
    "# Importing Libraries and Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "#reading in census population data for estimating market-size-dictated heterogeneous treatment effects\n",
    "#in theory, smaller market postings should see a bigger boost in interest from job remoteness\n",
    "census_data = (pd.read_csv('sub-est2021_all.csv', encoding='latin-1')[['NAME', 'POPESTIMATE2021']].\n",
    "               rename(columns={'NAME': 'Market', 'POPESTIMATE2021': 'Population'}))\n",
    "#a handful of hamlets, CDPs, and areas not properly addressed by my cleaning need to be manually specified\n",
    "missing_mkts = ['King of Prussia', 'Washington, DC', 'Reston', 'Carson City', 'Byron Center', \n",
    "                'Chantilly', 'Devens', 'Glen Burnie', 'Hopewell Junction', 'Monmouth Junction', \n",
    "                'Newtown Square', 'Plainsboro Center', 'South Hooksett', 'Whippany', 'Whitehouse Station', \n",
    "                'Melville', 'Westlake, TX', 'Highlands Ranch', 'Patuxent River', \n",
    "                'Annapolis Junction', 'Basking Ridge', 'Beale Air Force Base', 'Boise',\n",
    "                'Iselin', 'Landisville', 'Lutherville-Timonium', 'Prudhoe Bay',\n",
    "                'Purchase', 'St Louis Park']\n",
    "missing_pops = [22028, 712816, 61418, 58993, 6653, \n",
    "                23820, 2007, 69649, 725, 8911, \n",
    "                13726, 2715, 5911, 8863, 3838, \n",
    "                17992, 1683, 107017, 12934, 40, 26747, \n",
    "                69536, 237446, 17356, 4771, 15814, \n",
    "                1416, 5391, 49158]\n",
    "#an index starting at the end of the census dataframe\n",
    "idx = [81416 + i for i in range(len(missing_mkts))]\n",
    "#cities, towns and counties annoyingly have 'city', 'town', or 'county' at the end of each of their names\n",
    "#only need to know if they're county\n",
    "census_data['Market'] = census_data['Market'].apply(lambda x: x.rsplit(' ', 1)[0] if 'county' not in x.lower() else x)\n",
    "missing_mkts = pd.DataFrame({'Market': missing_mkts, 'Population': missing_pops}, index=idx)\n",
    "census_data = pd.concat([census_data, missing_mkts], axis=0)\n",
    "#coercing certain census data spellings to what's apparently used on LinkedIn\n",
    "census_data['Market'] = census_data['Market'].replace({'Nashville-Davidson metropolitan': 'Nashville', \n",
    "                                                       'Louisville/Jefferson County metro government (balance)': 'Louisville', \n",
    "                                                       'Bronx County': 'Bronx', \n",
    "                                                       'Cranberry': 'Cranberry Township', \n",
    "                                                       'Minneapolis': 'Minneapolis–Saint Paul', \n",
    "                                                       'St. Paul': 'Minneapolis–Saint Paul',\n",
    "                                                       'Parsippany-Troy Hills': 'Parsippany', \n",
    "                                                       'St. Louis': 'St Louis', \n",
    "                                                       'St. Petersburg': 'St Petersburg'})\n",
    "#considering the Twin Cities one market\n",
    "census_data.loc[census_data['Market'].str.contains('Minneapolis–Saint Paul'), 'Population'] = 425336+307193\n",
    "#manually fixing some other issues\n",
    "census_data.loc[census_data['Market'].str.contains('McLean'), 'Population'] = 48566\n",
    "census_data.loc[census_data['Market'].str.contains('Lexington'), 'Population'] = 321793\n",
    "census_data.loc[census_data['Market']=='Hershey', 'Population'] = 14941\n",
    "census_data.loc[census_data['Market'].str.contains('Wilmington'), 'Population'] = 70750\n",
    "census_data.loc[census_data['Market'].str.contains('Frankfort'), 'Population'] = 28595\n",
    "census_data.loc[census_data['Market']=='Fulton', 'Population'] = 2049\n",
    "#though need to see if any Columbus, OH end up in sample. Diff (bigger) pop. (see hybrid matched sample)\n",
    "census_data.loc[census_data['Market']=='Columbus', 'Population'] = 50569\n",
    "#there are plenty of duplicates across states; I'll keep the most populous since it's likeliest that's what's in my data\n",
    "census_data.sort_values(by=['Population'], ascending=False, inplace=True)\n",
    "census_data.drop_duplicates(subset=['Market'], keep='first', inplace=True)\n",
    "#I later audit the 20 or so most and least populous markets to make sure there are no glaring mistakes\n",
    "#e.g. San Diego city being assigned San Diego County's population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b56619",
   "metadata": {},
   "source": [
    "# Reading in Job-Posting Data and Some Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e802f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 14 files are all Easy Applys but I didn't track at the time, while remainder are a mix I tracked\n",
    "#I'm going to treat sentinel values 'Not Available' and 'Error' as NAs\n",
    "dfs = [pd.read_csv('li_postings.csv', na_values=['Not Available', 'Error']).assign(Easy_Apply=1)]\n",
    "for i in range(2,823):\n",
    "    if i < 15:\n",
    "        dfs.append(pd.read_csv(f'li_postings{i}.csv', na_values=['Not Available', 'Error']).assign(Easy_Apply=1))\n",
    "    else:\n",
    "        dfs.append(pd.read_csv(f'li_postings{i}.csv', na_values=['Not Available', 'Error']))\n",
    "data=pd.concat(dfs).reset_index(drop=True)\n",
    "#it's only worth dropping observations missing time up and/or applicant values, since those are crucial\n",
    "data = data[(data['Time Up'].notna())&(data['Applicants'].notna())]\n",
    "#also, I'm for now making the judgement call to exclude jobs posted less than an hour prior to scraping\n",
    "data = data[(~data['Time Up'].str.contains('second'))&(~data['Time Up'].str.contains('minute'))]\n",
    "#granularity/duplicate handling\n",
    "#all the postings\n",
    "count = data.shape[0]\n",
    "#number of postings unique at the Job Title, Company, and Location level\n",
    "unique_count = data[~data.duplicated(subset=['Job_Title', 'Company', 'Location'])].shape[0]\n",
    "print(f'{unique_count} of {count} rows are postings unique at the job title-company-location level.')\n",
    "\n",
    "#accounting for some number of postings advertised as remote or hybrid in the job title; a couple hundred or so\n",
    "def remoteness_from_title(df):\n",
    "    remote_words = ['Remote', 'Home', 'WFH', 'REMOTE', 'HOME']\n",
    "    hybrid_words = ['Hybrid', 'HYBRID']\n",
    "    if (any(word in df['Job_Title'] for word in remote_words)):\n",
    "        return 'Remote'\n",
    "    elif (any(word in df['Job_Title'] for word in hybrid_words)):\n",
    "        return 'Hybrid'\n",
    "    else:\n",
    "        return df['Remoteness']\n",
    "data['Remoteness'] = data.apply(remoteness_from_title, axis=1)\n",
    "raw = data.copy()\n",
    "raw['Remoteness'] = raw['Remoteness'].fillna('Not Mentioned')\n",
    "#another number of postings' work arrangements are only given in the description; I'm finding that those with \n",
    "#job descriptions containing the word 'hybrid' are hybrid, but that this is not necessarily the case for remote jobs, \n",
    "#as some hybrid jobs are described as 'hybrid remote' or something like 'remote 3 days a week onsite 2'. \n",
    "#Therefore, maybe I only mark jobs having 'remote' and not having that kind of language in their job description as remote. \n",
    "\n",
    "# def remoteness_from_desc(df):\n",
    "#     text = df['Job_Title'].lower()\n",
    "#     on_site_words = ['on-site', 'onsite', 'on site', 'office', 'hybrid']\n",
    "#     hybrid_filter = (df['Remoteness'].isna())&('hybrid' in text)\n",
    "#     remote_filter = (df['Remoteness'].isna())&('remote' in text)&(all(word not in text for word in on_site_words))\n",
    "#     if hybrid_filter:\n",
    "#         return 'Hybrid'\n",
    "#     elif remote_filter:\n",
    "#         return 'Remote'\n",
    "#     else:\n",
    "#         return df['Remoteness']\n",
    "# data['Remoteness'] = data.apply(remoteness_from_desc, axis=1)\n",
    "\n",
    "#for now, I'm going to treat companies not clearly advertised as remote or hybrid as on-site. \n",
    "data['Remoteness'] = data['Remoteness'].fillna('On-site')\n",
    "\n",
    "#standardizing job title capitalization for duplicate filtering later on\n",
    "data['Job_Title'] = data['Job_Title'].apply(lambda string: string.title())\n",
    "\n",
    "#casting Applicant Count to int; removing commas and needing to pitstop at float for some reason\n",
    "data['Applicants'] = data['Applicants'].astype(str)\n",
    "data['Applicants'] = data['Applicants'].apply(lambda x: x.replace(',', '') if ',' in x else x)\n",
    "data['Applicants'] = data['Applicants'].astype(float)\n",
    "data['Applicants'] = data['Applicants'].astype(int)\n",
    "\n",
    "#it might also be worth stripping those remote and hybrid words from the job title if\n",
    "#text features are used in a propensity score model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14037939",
   "metadata": {},
   "source": [
    "# More Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deriving work basis dummies\n",
    "data['Contract_Role'] = data.apply(lambda row: int('contract' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Part_Time_Role'] = data.apply(lambda row: int('part-time' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Full_Time_Role'] = data.apply(lambda row: int('full-time' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Missing_Work_Basis'] = data['Full_Time_Role']+data['Part_Time_Role']+data['Contract_Role']\n",
    "data['Missing_Work_Basis'] = data['Missing_Work_Basis'].replace({1: 0, 0: 1})\n",
    "\n",
    "#deriving seniority level dummies...\n",
    "data['Internship'] = data.apply(lambda row: int('internship' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Entry_Level'] = data.apply(lambda row: int('entry level' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Associate_Level'] = data.apply(lambda row: int('associate' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Mid_Senior_Level'] = data.apply(lambda row: int('mid-senior level' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Director_Level'] = data.apply(lambda row: int('director' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Executive_Level'] = data.apply(lambda row: int('executive' in str(row.Position_Info).lower()), axis=1)\n",
    "data['Missing_Seniority'] = (data['Internship']+data['Entry_Level']+data['Associate_Level']+data['Mid_Senior_Level']\n",
    "                             +data['Director_Level']+data['Executive_Level'])\n",
    "data['Missing_Seniority'] = data['Missing_Seniority'].replace({1: 0, 0: 1})\n",
    "#...and a single categorical column for ease of analysis later\n",
    "#about 70 rows have multiple seniorities listed but all are internships, so they're internships\n",
    "#see via data[~(data.iloc[:, 17:24].sum(axis=1)>1)]\n",
    "def seniority_puller(df):\n",
    "    if df['Internship']==1:\n",
    "        return 'Internship'\n",
    "    elif df['Entry_Level']==1:\n",
    "        return 'Entry Level'\n",
    "    elif df['Associate_Level']==1:\n",
    "        return 'Associate Level'\n",
    "    elif df['Mid_Senior_Level']==1:\n",
    "        return 'Mid-Senior Level'\n",
    "    elif df['Director_Level']==1:\n",
    "        return 'Director Level'\n",
    "    elif df['Executive_Level']==1:\n",
    "        return 'Executive Level'\n",
    "    else:\n",
    "        return 'Seniority Info Not Given'\n",
    "data['Seniority'] = data.apply(seniority_puller, axis=1)\n",
    "\n",
    "#deriving salary info\n",
    "data['Salary_Info'] = data.apply(lambda row: str(row['Position_Info']).split('·')[0] if '$' in str(row['Position_Info']) else 'Not Given', axis=1)\n",
    "data['Gives_Salary'] = data.apply(lambda row: 1 if str(row['Salary_Info'])!='Not Given' else 0, axis=1)\n",
    "def salary_cleaner(info, end='low'):\n",
    "    if info.count('$')==2:\n",
    "        if end=='low':\n",
    "            return info.split('-')[0].split('/')[0].strip('$').replace(',', '')\n",
    "        else:\n",
    "            return info.split('-')[1].split('/')[0].strip().strip('$').replace(',', '')\n",
    "    elif info=='Not Given':\n",
    "        return '0' #though technically N/A\n",
    "    else:\n",
    "        return info.split('/')[0].strip('$').replace(',', '')\n",
    "def salary_basis(info):\n",
    "    if '/yr' in info.lower():\n",
    "        return 'Year'\n",
    "    elif '/hr' in info.lower():\n",
    "        return 'Hour'\n",
    "    elif '/month' in info.lower():\n",
    "        return 'Month'\n",
    "    else:\n",
    "        return 'Not Clear'\n",
    "#the following ensures no salary info is of some form other than a range or single number\n",
    "if len(np.array(pd.Series(data.apply(lambda row: row.Salary_Info.count('$'), axis=1)).value_counts().index)) == 3:\n",
    "    data['Pay_Floor'] = data.apply(lambda row: salary_cleaner(row['Salary_Info'], end='low'), axis=1)\n",
    "    data['Pay_Ceiling'] = data.apply(lambda row: salary_cleaner(row['Salary_Info'], end='high'), axis=1)\n",
    "    data['Pay_Basis'] = data.apply(lambda row: salary_basis(row['Salary_Info']), axis=1)\n",
    "\n",
    "data['Pay_Floor'] = data['Pay_Floor'].astype(float)\n",
    "data['Pay_Ceiling'] = data['Pay_Ceiling'].astype(float)\n",
    "data['Pay_Floor_Annual'] = data.apply(lambda row: row['Pay_Floor']*40*52.143 if row['Pay_Basis']=='Hour' else row['Pay_Floor'], axis=1)\n",
    "data['Pay_Ceiling_Annual'] = data.apply(lambda row: row['Pay_Ceiling']*40*52.143 if row['Pay_Basis']=='Hour' else row['Pay_Ceiling'], axis=1)\n",
    "data['Pay_Info_Annual_Mean'] = (data['Pay_Floor_Annual']+data['Pay_Ceiling_Annual'])/2\n",
    "#deriving applicant count per day\n",
    "\n",
    "#converting to timedelta\n",
    "def to_timedelta(a_str):\n",
    "    day_seconds = 86400\n",
    "    quant = int(a_str.split(' ')[0])\n",
    "    if 'hour' in a_str:\n",
    "        return timedelta(hours=quant).seconds / day_seconds\n",
    "    elif 'day' in a_str:\n",
    "        return timedelta(days=quant).days\n",
    "    elif 'week' in a_str:\n",
    "        return timedelta(weeks=quant).days\n",
    "    elif 'month' in a_str:\n",
    "        #convert to avg no. of weeks in a month\n",
    "        quant*=4.33\n",
    "        return timedelta(weeks=quant).days\n",
    "data['Days_Up'] = data.apply(lambda row: to_timedelta(row['Time Up']), axis=1)\n",
    "#casting Applicant Count to int and getting rate\n",
    "data['Applicants'] = data['Applicants'].astype(str)\n",
    "data['Applicants'] = data['Applicants'].apply(lambda x: x.replace(',', '') if ',' in x else x)\n",
    "data['Applicants'] = data['Applicants'].astype(float)\n",
    "data['Applicants'] = data['Applicants'].astype(int)\n",
    "data['Apps_per_Day'] = data['Applicants'] / data['Days_Up']\n",
    "\n",
    "#pulling company size and industry\n",
    "def employee_counter(string):\n",
    "    if ('-' in string) | ('+' in string):\n",
    "        if '·' in string:\n",
    "            return string.split('·')[0]\n",
    "        else:\n",
    "            return string\n",
    "    else:\n",
    "        return 'Not Given'\n",
    "\n",
    "def industry_puller(string):\n",
    "    if ('-' in string) | ('+' in string):\n",
    "        if '·' in string:\n",
    "            return string.split('·')[1]\n",
    "        else:\n",
    "            return 'Not Given'\n",
    "    elif 'Not Available' in string:\n",
    "        return 'Not Given'\n",
    "    else:\n",
    "        return string\n",
    "data['Company_Size'] = data.apply(lambda row: employee_counter(str(row['Company_Info'])), axis=1)\n",
    "data['Company_Size'] = data['Company_Size'].apply(lambda x: x.strip())\n",
    "data['Industry'] = data.apply(lambda row: industry_puller(str(row['Company_Info'])), axis=1)\n",
    "data['Industry'] = data['Industry'].apply(lambda x: x.strip())\n",
    "\n",
    "#pulling location info\n",
    "def city_area_puller(string):\n",
    "    if 'Washington, DC' in string:\n",
    "        return 'Washington, DC'\n",
    "    elif 'District of Columbia' in string:\n",
    "        return 'Washington, DC'\n",
    "    elif 'Washington DC-Baltimore Area' in string:\n",
    "        return 'Washington, DC'\n",
    "    elif 'Hartford, CT' in string:\n",
    "        return 'Hartford'\n",
    "    elif ('Minneapolis' in string) | ('Paul, MN' in string):\n",
    "        return 'Minneapolis–Saint Paul'\n",
    "    elif 'Westlake, TX' in string:\n",
    "        return 'Westlake, TX'\n",
    "    elif ',' in string:\n",
    "        return string.split(',')[0]\n",
    "    elif 'area' in string.lower():\n",
    "        if 'salt lake' in string.lower():\n",
    "            return string.replace('Area', '').replace('Metropolitan', '').replace('Bay', '').replace('County', '').replace('Greater', '').strip()\n",
    "        else:\n",
    "            return string.replace('Area', '').replace('Metropolitan', '').replace('City', '').replace('County', '').replace('Greater', '').replace('Bay', '').strip() \n",
    "    else:\n",
    "        return 'Not Available'\n",
    "        \n",
    "def state_puller(string):\n",
    "    if ',' in string:\n",
    "        if string.split(',')[1]!='United States':\n",
    "            return string.split(',')[1]\n",
    "        else:\n",
    "            return 'Not Available'\n",
    "    elif string == 'United States':\n",
    "        return 'Not Available'\n",
    "    else:\n",
    "        return 'Further Parsing Needed'\n",
    "\n",
    "data['Market'] = data.apply(lambda row: city_area_puller(row['Location']), axis=1)\n",
    "data['Market'] = np.where(data['Market'].str.contains('San Francisco'), 'San Francisco', data['Market'])\n",
    "data['Market'] = np.where(data['Market'].str.contains('New York'), 'New York', data['Market'])\n",
    "data['Market'] = np.where(data['Market'].str.contains('Los Angeles'), 'Los Angeles', data['Market'])\n",
    "data['Market'] = np.where(data['Market'].str.contains('Phoenix'), 'Phoenix', data['Market'])\n",
    "data['State'] = data.apply(lambda row: state_puller(row['Location']), axis=1)\n",
    "data['State'] = data['State'].apply(lambda x: x.strip())\n",
    "data['Market'] = data['Market'].apply(lambda x: x.strip())\n",
    "\n",
    "#pulling job requirements info; I first look for mentions of any degree before then only caring about highest\n",
    "def degree_finder(string, *keywords):\n",
    "    if any(word in string.lower() for word in keywords):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "associate_degree_keywords = ['associate degree', 'associates degree', 'associate\\'s degree', 'junior college degree']\n",
    "bachelor_degree_keywords = ['bachelor', 'bachelors', 'bachelor\\'s']\n",
    "master_degree_keywords = ['master', 'masters', 'master\\'s', 'advanced degree', 'graduate degree']\n",
    "phd_keywords = ['phd', 'p.h.d', 'doctorate', 'doctor of philosophy', 'ph.d', 'dphil', 'professional degree', 'postdoc']\n",
    "data['Associate_D'] = data.apply(lambda row: degree_finder(str(row['Job_Description']), *associate_degree_keywords), axis=1)\n",
    "data['Bachelor_D'] = data.apply(lambda row: degree_finder(str(row['Job_Description']), *bachelor_degree_keywords), axis=1)\n",
    "data['Master_D'] = data.apply(lambda row: degree_finder(str(row['Job_Description']), *master_degree_keywords), axis=1)\n",
    "data['PhD'] = data.apply(lambda row: degree_finder(str(row['Job_Description']), *phd_keywords), axis=1)\n",
    "#adding a highest education column\n",
    "def highest_education(df):\n",
    "    if df['PhD']==1:\n",
    "        return 'PhD'\n",
    "    elif df['Master_D']==1:\n",
    "        return 'Master\\'s'\n",
    "    elif df['Bachelor_D']==1:\n",
    "        return 'Bachelor\\'s'\n",
    "    elif df['Associate_D']==1:\n",
    "        return 'Associate\\'s'\n",
    "    else:\n",
    "        return 'No Higher Education Mentioned'\n",
    "data['Highest_Education'] = data.apply(highest_education, axis=1)\n",
    "#dropping those dummies\n",
    "data = data.drop(columns=['Associate_D', 'Bachelor_D', 'Master_D', 'PhD'])\n",
    "#adding new dummies\n",
    "data = pd.concat([data, pd.get_dummies(data['Highest_Education'])], axis=1)\n",
    "\n",
    "\n",
    "#benefits columns\n",
    "data['four0onek'] = data.apply(lambda row: 1 if '401' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['medical_insurance'] = data.apply(lambda row: 1 if 'medical' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['vision_insurance'] = data.apply(lambda row: 1 if 'vision' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['dental_insurance'] = data.apply(lambda row: 1 if 'dental' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['disability_insurance'] = data.apply(lambda row: 1 if 'disability' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['paid_maternity_leave'] = data.apply(lambda row: 1 if 'paid maternity' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['paid_paternity_leave'] = data.apply(lambda row: 1 if 'paid_paternity' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['tuition_assistance'] = data.apply(lambda row: 1 if 'tuition' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['commuter_benefits'] = data.apply(lambda row: 1 if 'commuter' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "data['pension'] = data.apply(lambda row: 1 if 'pension' in str(row['Benefits']).lower() else 0, axis=1)\n",
    "\n",
    "#################################################ongoing efforts to parse out years of experience###################\n",
    "\n",
    "#removing 'years of age' and 'years old' and turning text to digits to more easily identify years experience requirements\n",
    "#lowercase text to be turned into digits\n",
    "lc_numbers = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', \n",
    "'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen']\n",
    "#uppercase text to be turned into digits\n",
    "uc_numbers = [number.title() for number in lc_numbers]\n",
    "lc_numbers = np.array(lc_numbers)\n",
    "uc_numbers = np.array(uc_numbers)\n",
    "#dictionaries to refer to for text-to-digit substitution\n",
    "lc_digit_dict = {n: i for i, n in zip(np.arange(18).astype(str), lc_numbers)}\n",
    "uc_digit_dict = {n: i for i, n in zip(np.arange(18).astype(str), uc_numbers)}\n",
    "\n",
    "#this function will remove references to age in job descriptions and change text numbers to number numbers\n",
    "def year_digit_processor(string):\n",
    "    string = re.sub(\"years of age\", '', string)\n",
    "    string = re.sub(\"years old\", '', string)\n",
    "    \n",
    "    #a boolean array indicating what if any text numbers were found\n",
    "    lc_boolean = np.array(list(number in string for number in lc_numbers))\n",
    "    if any(lc_boolean): #iterate through each and replace them\n",
    "        for text in lc_numbers[lc_boolean]:\n",
    "            string = re.sub(text, lc_digit_dict[text], string)\n",
    "    else:\n",
    "        pass\n",
    "    #same thing for uppercase\n",
    "    uc_boolean = np.array(list(number in string for number in uc_numbers))\n",
    "    if any(uc_boolean):\n",
    "        for text in uc_numbers[uc_boolean]:\n",
    "            string = re.sub(text, uc_digit_dict[text], string)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return string\n",
    "data['Job_Description'] = data['Job_Description'].fillna('None Given')\n",
    "data['Job_Description'] = data.apply(lambda row: year_digit_processor(row['Job_Description']), axis=1)\n",
    "\n",
    "#regular expressions for identifying years of experience requirements\n",
    "patterns_pt_1 = r'(\\D[0-9] or more year)|(\\D[0-9] plus year)|(\\D[0-9]-plus year)|(\\D[0-9] year)|(\\D[0-9][+] year)|'\n",
    "patterns_pt_2 = r'(1[0-7][+] year)|(1[0-7] or more years)|(1[0-7] plus years)|(1[0-7]-plus years)|(1[0-7] years)|'\n",
    "patterns_pt_3 = r'([0-9][.]5 year)|(\\D[0-9]-year)|(\\D[0-9]\\) year)|(\\D1[0-7]\\) year)|(inimum [0-9])|(\\D[0-9] [+] year)|'\n",
    "patterns_pt_4 = r'(\\D1[0-7] [+] year)|(inimum [0-9]-[0-9])|(inimum [0-9]-1[0-9])|(inimum 1[0-9]-1[0-9]|)'\n",
    "patterns_pt_5 = r'(inimum [0-9] to [0-9])|(inimum [0-9] to 1[0-9])|(inimum 1[0-9] to 1[0-9])'\n",
    "yr_patterns = patterns_pt_1+patterns_pt_2+patterns_pt_3+patterns_pt_4+patterns_pt_5\n",
    "\n",
    "#a function that will take in a string following any of the above patterns, remove nuisance characters, and return a number\n",
    "def year_cleaner(string):\n",
    "    string = string.lower()\n",
    "    if 'inimum' not in string:#if it wasn't a 'minimum' pattern, first replace all nuisance chracters with spaces\n",
    "        string = re.sub(\"[~、\\•<>≥()+-,-*'/t:\\n–]\", ' ', string).strip().strip('-').strip('[').strip(']')\n",
    "        #then isolate the actual number, dropping any remaining nuisance characters\n",
    "        fig = string.split(' ')[0]\n",
    "        fig = re.sub(\"[A-Za-z>()+-,*'/t]\", '', fig)\n",
    "        #turn to a float\n",
    "        return float(fig)\n",
    "    else: #if it was a minimum pattern, strip that part of the string and convert to a float. \n",
    "        return float(string.strip('inimum').strip())\n",
    "        \n",
    "#this function is what's utilizing the above patterns. \n",
    "def reqd_exper_parser(patterns, string):\n",
    "    #creating a list with entries for every pattern identified\n",
    "    finds = re.findall(patterns, string)\n",
    "    try: #there are some intractable formats to be accounted for later on\n",
    "        if len(finds)!=0: #if any patterns were identified\n",
    "            #flatten the list. It'll be mostly empty because unidentified patterns are indicated by empty string\n",
    "            yrs_of_exp = [list(x) for x in finds]\n",
    "            yrs_of_exp = [elem for a_list in yrs_of_exp for elem in a_list]\n",
    "            #clean non-empty entries to compute the maximum years requirement listed\n",
    "            #'if len(year)!=0 is to skip the empty entries'\n",
    "            yrs_of_exp = np.array([year_cleaner(year.strip('‐')) for year in yrs_of_exp if len(year)!=0], dtype=float)\n",
    "            return np.max(yrs_of_exp)\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 'Something went wrong'\n",
    "data['Required_Years_Experience'] = data.apply(lambda row: reqd_exper_parser(yr_patterns, row['Job_Description']), axis=1)\n",
    "#################################################removing duplicates###################\n",
    "\n",
    "#de-duplicating to uniqueness at the job title, company, remoteness, and location level\n",
    "#first sorting by those columns and days up in ascending order\n",
    "#then removing duplicates based on which were scraped first\n",
    "uniqueness_identifiers = ['Job_Title', 'Company', 'Remoteness', 'Location']\n",
    "data.sort_values(by=uniqueness_identifiers+['Days_Up'], ascending=True, inplace=True)\n",
    "data = data.drop_duplicates(subset=uniqueness_identifiers, keep='last')\n",
    "\n",
    "#disregarding issue for now and casting as numeric\n",
    "data = data[data['Required_Years_Experience']!='Something went wrong']\n",
    "data['Required_Years_Experience'] = data['Required_Years_Experience'].astype(float)\n",
    "\n",
    "#outcome variable\n",
    "data['Apps_per_Day_log'] = np.log(data['Apps_per_Day']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0c988",
   "metadata": {},
   "source": [
    "# Remote/On-Site Matching Sans Job Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24097e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering to only on-site and remote postings with information on market, industry, and company size\n",
    "df_rm_os = data.copy()[(data['Remoteness']!='Hybrid')]\n",
    "df_rm_os = df_rm_os[(df_rm_os['Market']!='Not Available')&(df_rm_os['Industry']!='Not Given')]\n",
    "#treatment indicator\n",
    "df_rm_os['Remote'] = np.where(df_rm_os['Remoteness']=='Remote', 1, 0)\n",
    "#binning posting age, informed by the above visualization\n",
    "df_rm_os['Posting_Age_Bin'] = pd.cut(df_rm_os['Days_Up'], [0, 1.0, 212.0])\n",
    "df_rm_os['Posting_Age_Bin'] = df_rm_os['Posting_Age_Bin'].astype(str)\n",
    "#binning mean salary infos -- most are 0\n",
    "df_rm_os['Pay_Info_Annual_Mean'] = (df_rm_os['Pay_Floor_Annual']+df_rm_os['Pay_Ceiling_Annual'])/2\n",
    "df_rm_os['Pay_Info_Mean_Category'] = (pd.cut(df_rm_os['Pay_Info_Annual_Mean'], \n",
    "                                               bins=[i*10000 for i in range(26)], \n",
    "                                               include_lowest=True)).astype(str)\n",
    "df_rm_os['Pay_Info_Mean_Category'] = np.where(df_rm_os['Pay_Info_Annual_Mean']>250000, 'Over $250,000', df_rm_os['Pay_Info_Mean_Category'])\n",
    "df_rm_os['Pay_Info_Mean_Category'] = np.where(df_rm_os['Pay_Ceiling_Annual']==0, 0, df_rm_os['Pay_Info_Mean_Category'])\n",
    "#binning years experience requirement info\n",
    "df_rm_os['yrs_exp_bin'] = pd.cut(df_rm_os['Required_Years_Experience'], \n",
    "                                   bins=[i*3 for i in range(4)], \n",
    "                                   include_lowest=True)\n",
    "df_rm_os['yrs_exp_bin'] = np.where(df_rm_os['Required_Years_Experience']>=10, '10 or more', df_rm_os['yrs_exp_bin'])\n",
    "#creating a list of 6 matching variables\n",
    "matching_variables1 = ['Market', 'Industry', 'Company_Size', 'Pay_Info_Mean_Category', 'yrs_exp_bin', 'Posting_Age_Bin']\n",
    "#grouping by that list AND treatment status, then ID'ing duplicates based on matching variables, we can \n",
    "#detect where in the covariate space potential matches are present (before gauging text similarity)\n",
    "matchspots1 = (df_rm_os.groupby(matching_variables1 + ['Remote'])['Job_Title'].\n",
    "              count().reset_index()).rename(columns={'Job_Title': 'Observations'})\n",
    "areas = matchspots1.copy()\n",
    "matchspots1 = (matchspots1[matchspots1.duplicated(subset=matching_variables1)]\n",
    "              .drop(columns=['Remote', 'Observations']))\n",
    "matches = [] #to populate with matches\n",
    "match_id = 0 #to keep track of what matched with what\n",
    "for matchspot in range(len(matchspots1)): #i.e. for each row of promising covariate values\n",
    "    #filter the data to observations in that covariate space via a one-to-many inner join\n",
    "    df = matchspots1.iloc[matchspot, :].to_frame().T\n",
    "    df = df_rm_os.join(df.set_index(matching_variables1), \n",
    "                       on=matching_variables1, \n",
    "                       how='inner', \n",
    "                       lsuffix='_l', \n",
    "                       rsuffix='_r').reset_index(drop=True)\n",
    "    #I'll cycle through each treated unit and give it a control unit\n",
    "    treated_df = df.query('Remote==1').reset_index(drop=True)\n",
    "    control_df = df.query('Remote==0').reset_index(drop=True)\n",
    "    poss_matches = np.min([len(control_df), len(treated_df)])\n",
    "    for i in range(poss_matches):\n",
    "        matches.append(treated_df.iloc[i, :].to_frame().T.assign(Match_Id=match_id))\n",
    "        matches.append(control_df.iloc[i, :].to_frame().T.assign(Match_Id=match_id))\n",
    "        match_id += 1 #increment the match identifier\n",
    "matches = pd.concat(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches['Apps_per_Day_log'] = matches['Apps_per_Day_log'].astype(float)\n",
    "matches['Remote'] = matches['Remote'].astype(int)\n",
    "matches['Promoted'] = matches['Promoted'].astype(int)\n",
    "matches['Easy_Apply'] = matches['Easy_Apply'].astype(int)\n",
    "smf.ols('Apps_per_Day_log~Remote+Promoted+Easy_Apply', matches).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09f87e",
   "metadata": {},
   "source": [
    "# Remote/On-Site Matching W/ Job Title: tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ede90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a quick and dirty way of ensuring references to work arrangement in job titles (e.g. 'remote opportunity')\n",
    "#aren't used in matching\n",
    "def title_cleaner(string):\n",
    "    return (string.replace('Remote', '').replace('Home', '').replace('WFH', '').replace('REMOTE', '')\n",
    "            .replace('HOME', '').replace('Hybrid', '').replace('HYBRID', '')\n",
    "            .replace('remote', '').replace('hybrid', '').replace('wfh', '').replace('(', '').replace(')', '').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf538b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering to only on-site and remote postings with information on market, industry, and company size\n",
    "df_rm_os = data.copy()[(data['Remoteness']!='Hybrid')]\n",
    "df_rm_os = df_rm_os[(df_rm_os['Market']!='Not Available')&(df_rm_os['Industry']!='Not Given')]\n",
    "#treatment indicator\n",
    "df_rm_os['Remote'] = np.where(df_rm_os['Remoteness']=='Remote', 1, 0)\n",
    "#binning posting age, informed by the above visualization\n",
    "df_rm_os['Posting_Age_Bin'] = pd.cut(df_rm_os['Days_Up'], [0, 1.0, 212.0])\n",
    "df_rm_os['Posting_Age_Bin'] = df_rm_os['Posting_Age_Bin'].astype(str)\n",
    "#binning mean salary infos -- most are 0\n",
    "df_rm_os['Pay_Info_Annual_Mean'] = (df_rm_os['Pay_Floor_Annual']+df_rm_os['Pay_Ceiling_Annual'])/2\n",
    "df_rm_os['Pay_Info_Mean_Category'] = (pd.cut(df_rm_os['Pay_Info_Annual_Mean'], \n",
    "                                               bins=[i*10000 for i in range(26)], \n",
    "                                               include_lowest=True)).astype(str)\n",
    "df_rm_os['Pay_Info_Mean_Category'] = np.where(df_rm_os['Pay_Info_Annual_Mean']>250000, 'Over $250,000', df_rm_os['Pay_Info_Mean_Category'])\n",
    "df_rm_os['Pay_Info_Mean_Category'] = np.where(df_rm_os['Pay_Ceiling_Annual']==0, 0, df_rm_os['Pay_Info_Mean_Category'])\n",
    "#binning years experience requirement info\n",
    "df_rm_os['yrs_exp_bin'] = pd.cut(df_rm_os['Required_Years_Experience'], \n",
    "                                   bins=[i*3 for i in range(4)], \n",
    "                                   include_lowest=True)\n",
    "df_rm_os['yrs_exp_bin'] = np.where(df_rm_os['Required_Years_Experience']>=10, '10 or more', df_rm_os['yrs_exp_bin'])\n",
    "#creating a list of 6 matching variables\n",
    "matching_variables1 = ['Market', 'Industry', 'Company_Size', 'Pay_Info_Mean_Category', 'yrs_exp_bin', 'Posting_Age_Bin']\n",
    "#grouping by that list AND treatment status, then ID'ing duplicates based on matching variables, we can \n",
    "#detect where in the covariate space potential matches are present (before gauging text similarity)\n",
    "matchspots1 = (df_rm_os.groupby(matching_variables1 + ['Remote'])['Job_Title'].\n",
    "              count().reset_index()).rename(columns={'Job_Title': 'Observations'})\n",
    "matchspots1 = (matchspots1[matchspots1.duplicated(subset=matching_variables1)]\n",
    "              .drop(columns=['Remote', 'Observations']))\n",
    "\n",
    "#Now finally I'll trawl through those covariate spaces and log t/c pairings similar enough on title as matches\n",
    "matches = [] #to populate with matches\n",
    "match_id = 0 #to keep track of what matched with what\n",
    "\n",
    "for matchspot in range(len(matchspots1)): #i.e. for each row of promising covariate values\n",
    "    #filter the data to observations in that covariate space via a one-to-many inner join\n",
    "    df = matchspots1.iloc[matchspot, :].to_frame().T\n",
    "    df = df_rm_os.join(df.set_index(matching_variables1), \n",
    "                       on=matching_variables1, \n",
    "                       how='inner', \n",
    "                       lsuffix='_l', \n",
    "                       rsuffix='_r').reset_index(drop=True)\n",
    "    #record a cutoff pt. between existing and forthcoming columns; only the latter will be used to gauge similarity\n",
    "    col_count = df.shape[1]\n",
    "    #a vector of titles, stripped of any references to treatment with the title_cleaner function\n",
    "    titles = [title_cleaner(title) for title in df['Job_Title'].copy()]\n",
    "    #initializing a model that records all unigrams as features,  fitting it on titles, and cleaning future col names\n",
    "    vect = TfidfVectorizer(ngram_range=(1,1))\n",
    "    vect.fit(titles)\n",
    "    feats = [feat.replace(' ', '') for feat in vect.get_feature_names()]\n",
    "    #appending each posting's new features; use of Jaccard distance to gauge similarity necessitates Boolean\n",
    "    titles_matrix = vect.transform(titles)\n",
    "    text_df = pd.DataFrame(titles_matrix.toarray().astype(bool), columns=feats).reset_index(drop=True)\n",
    "    df = pd.concat([df, text_df], axis=1)\n",
    "    #features in hand, I'll cycle through each treated unit in the covariate space and find\n",
    "    #the control neighbor nearest to it based on those features (conditional on exact-matching legwork already done)\n",
    "    treated_df = df.query('Remote==1').reset_index(drop=True)\n",
    "    control_df = df.query('Remote==0').reset_index(drop=True)\n",
    "    #the model is re-fit for every treated unit in case I want to match without replacement,\n",
    "    #though the order-dependence of that might really hurt/destabilize the matching process\n",
    "    for i in range(len(treated_df)):\n",
    "        #a control unit model; predictions with it surface control neighbors\n",
    "        #(outcome variable Apps per Day not used)\n",
    "        control_neighbor_finder = (NearestNeighbors(n_neighbors=1).\n",
    "                                   fit(np.array(control_df.iloc[:, col_count:]), control_df['Apps_per_Day'])\n",
    "                                  )\n",
    "        #the treated unit's text features as boolean values\n",
    "        tdatum = np.array(treated_df.iloc[i, col_count:].to_frame().T, dtype=bool)\n",
    "        #the treated unit's distance from its nearest neighbor and the NN itself's index value in control_df\n",
    "        dist, neigh_loc = control_neighbor_finder.kneighbors(tdatum, n_neighbors=1)\n",
    "        dist, neigh_loc = dist[0][0], neigh_loc[0][0] #extracting scalar values\n",
    "        #if it's pretty close in job title to a control title in the covariate space, record both\n",
    "        if dist < 2: #this is subjective. But I think strict enough using Jaccard distance\n",
    "            matches.append(treated_df.iloc[i, :col_count].to_frame().T.assign(Title_Match_Id=match_id))\n",
    "            matches.append(control_df.iloc[neigh_loc, :col_count].to_frame().T.assign(Title_Match_Id=match_id))\n",
    "            match_id += 1 #increment the match identifier\n",
    "        else:\n",
    "            pass #since the treated unit's possible match was a poor one, discard it\n",
    "#turn the list of matched observations into a dataframe\n",
    "matches1 = pd.concat(matches)\n",
    "t1 = matches1.query(\"Remote==1\")\n",
    "c1 = matches1.query(\"Remote==0\")\n",
    "print('The sample size is now', len(matches1))\n",
    "print(f'However, this is composed of {len(t1)} treated units')\n",
    "unique_c_count = c1[~c1.duplicated(subset=['Job_Title', 'Time Up', 'Date_Scraped'])].shape[0]\n",
    "print(f'and {unique_c_count} control units.')\n",
    "#re-casting outcome / treatment variables\n",
    "matches1['Apps_per_Day_log'] = matches1['Apps_per_Day_log'].astype(float)\n",
    "matches1['Remote'] = matches1['Remote'].astype(int)\n",
    "matches1['Promoted'] = matches1['Promoted'].astype(int)\n",
    "matches1['Easy_Apply'] = matches1['Easy_Apply'].astype(int)\n",
    "smf.ols('Apps_per_Day_log~Remote+Easy_Apply+Promoted', matches1).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d446200",
   "metadata": {},
   "source": [
    "# Remote/On-Site Matching W/ Job Title: Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef040b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering to only on-site and remote postings with information on market, industry, and company size\n",
    "df_rm_os = data.copy()[(data['Remoteness']!='Hybrid')]\n",
    "df_rm_os = df_rm_os[(df_rm_os['Market']!='Not Available')&(df_rm_os['Industry']!='Not Given')]\n",
    "#treatment indicator\n",
    "df_rm_os['Remote'] = np.where(df_rm_os['Remoteness']=='Remote', 1, 0)\n",
    "#binning posting age, informed by the above visualization\n",
    "df_rm_os['Posting_Age_Bin'] = pd.cut(df_rm_os['Days_Up'], [0, 1.0, 212.0])\n",
    "df_rm_os['Posting_Age_Bin'] = df_rm_os['Posting_Age_Bin'].astype(str)\n",
    "#binning mean salary infos -- most are 0\n",
    "df_rm_os['Pay_Info_Annual_Mean'] = (df_rm_os['Pay_Floor_Annual']+df_rm_os['Pay_Ceiling_Annual'])/2\n",
    "df_rm_os['Pay_Info_Mean_Category'] = (pd.cut(df_rm_os['Pay_Info_Annual_Mean'], \n",
    "                                               bins=[i*10000 for i in range(26)], \n",
    "                                               include_lowest=True)).astype(str)\n",
    "df_rm_os['Pay_Info_Mean_Category'] = np.where(df_rm_os['Pay_Info_Annual_Mean']>250000, 'Over $250,000', df_rm_os['Pay_Info_Mean_Category'])\n",
    "df_rm_os['Pay_Info_Mean_Category'] = np.where(df_rm_os['Pay_Ceiling_Annual']==0, 0, df_rm_os['Pay_Info_Mean_Category'])\n",
    "#binning years experience requirement info\n",
    "df_rm_os['yrs_exp_bin'] = pd.cut(df_rm_os['Required_Years_Experience'], \n",
    "                                   bins=[i*3 for i in range(4)], \n",
    "                                   include_lowest=True)\n",
    "df_rm_os['yrs_exp_bin'] = np.where(df_rm_os['Required_Years_Experience']>=10, '10 or more', df_rm_os['yrs_exp_bin'])\n",
    "#creating a list of 6 matching variables\n",
    "matching_variables1 = ['Market', 'Industry', 'Company_Size', 'Pay_Info_Mean_Category', 'yrs_exp_bin', 'Posting_Age_Bin']\n",
    "#grouping by that list AND treatment status, then ID'ing duplicates based on matching variables, we can \n",
    "#detect where in the covariate space potential matches are present (before gauging text similarity)\n",
    "matchspots1 = (df_rm_os.groupby(matching_variables1 + ['Remote'])['Job_Title'].\n",
    "              count().reset_index()).rename(columns={'Job_Title': 'Observations'})\n",
    "matchspots1 = (matchspots1[matchspots1.duplicated(subset=matching_variables1)]\n",
    "              .drop(columns=['Remote', 'Observations']))\n",
    "\n",
    "#Now finally I'll trawl through those covariate spaces and log t/c pairings similar enough on title as matches\n",
    "matches = [] #to populate with matches\n",
    "match_id = 0 #to keep track of what matched with what\n",
    "\n",
    "for matchspot in range(len(matchspots1)): #i.e. for each row of promising covariate values\n",
    "    #filter the data to observations in that covariate space via a one-to-many inner join\n",
    "    df = matchspots1.iloc[matchspot, :].to_frame().T\n",
    "    df = df_rm_os.join(df.set_index(matching_variables1), \n",
    "                       on=matching_variables1, \n",
    "                       how='inner', \n",
    "                       lsuffix='_l', \n",
    "                       rsuffix='_r').reset_index(drop=True)\n",
    "    #record a cutoff pt. between existing and forthcoming columns; only the latter will be used to gauge similarity\n",
    "    col_count = df.shape[1]\n",
    "    #a vector of titles, stripped of any references to treatment with the title_cleaner function\n",
    "    titles = [title_cleaner(title) for title in df['Job_Title'].copy()]\n",
    "    #initializing a model that records all unigrams as features,  fitting it on titles, and cleaning future col names\n",
    "    vect = CountVectorizer(ngram_range=(1,1))\n",
    "    vect.fit(titles)\n",
    "    feats = [feat.replace(' ', '') for feat in vect.get_feature_names()]\n",
    "    #appending each posting's new features; use of Jaccard distance to gauge similarity necessitates Boolean\n",
    "    titles_matrix = vect.transform(titles)\n",
    "    text_df = pd.DataFrame(titles_matrix.toarray().astype(bool), columns=feats).reset_index(drop=True)\n",
    "    df = pd.concat([df, text_df], axis=1)\n",
    "    #features in hand, I'll cycle through each treated unit in the covariate space and find\n",
    "    #the control neighbor nearest to it based on those features (conditional on exact-matching legwork already done)\n",
    "    treated_df = df.query('Remote==1').reset_index(drop=True)\n",
    "    control_df = df.query('Remote==0').reset_index(drop=True)\n",
    "    #the model is re-fit for every treated unit in case I want to match without replacement,\n",
    "    #though the order-dependence of that might really hurt/destabilize the matching process\n",
    "    for i in range(len(treated_df)):\n",
    "        #a control unit model; predictions with it surface control neighbors\n",
    "        #(outcome variable Apps per Day not used)\n",
    "        control_neighbor_finder = (NearestNeighbors(n_neighbors=1, metric='jaccard').\n",
    "                                   fit(np.array(control_df.iloc[:, col_count:]), control_df['Apps_per_Day'])\n",
    "                                  )\n",
    "        #the treated unit's text features as boolean values\n",
    "        tdatum = np.array(treated_df.iloc[i, col_count:].to_frame().T, dtype=bool)\n",
    "        #the treated unit's distance from its nearest neighbor and the NN itself's index value in control_df\n",
    "        dist, neigh_loc = control_neighbor_finder.kneighbors(tdatum, n_neighbors=1)\n",
    "        dist, neigh_loc = dist[0][0], neigh_loc[0][0] #extracting scalar values\n",
    "        #if it's pretty close in job title to a control title in the covariate space, record both\n",
    "        if dist < .6: #this is subjective. But I think strict enough using Jaccard distance\n",
    "            matches.append(treated_df.iloc[i, :col_count].to_frame().T.assign(Title_Match_Id=match_id))\n",
    "            matches.append(control_df.iloc[neigh_loc, :col_count].to_frame().T.assign(Title_Match_Id=match_id))\n",
    "            match_id += 1 #increment the match identifier\n",
    "        else:\n",
    "            pass #since the treated unit's possible match was a poor one, discard it\n",
    "#turn the list of matched observations into a dataframe\n",
    "matches2 = pd.concat(matches)\n",
    "#finally, matching completed, it's maybe worth noting how small the unique set of control observations is\n",
    "#if way smaller, we're relying on a relatively small set of control units, increasing variance\n",
    "#a way to offset this is to increase the distance threshold (e.g. from .6 to .7), which of course will increase bias\n",
    "#another way is to match without replacement, which also increases bias because this also worsens average match quality\n",
    "t1 = matches2.query(\"Remote==1\")\n",
    "c1 = matches2.query(\"Remote==0\")\n",
    "print('The sample size is now', len(matches2))\n",
    "print(f'However, this is composed of {len(t1)} treated units')\n",
    "unique_c_count = c1[~c1.duplicated(subset=['Job_Title', 'Time Up', 'Date_Scraped'])].shape[0]\n",
    "print(f'and {unique_c_count} control units.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cec281",
   "metadata": {},
   "source": [
    "#### Some More Processing+Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining market sizes according to Census data\n",
    "matches2 = matches2.merge(census_data, on='Market', how='left')\n",
    "matches2['Population_thousands'] = matches2['Population']/1000\n",
    "matches2['Population_hundred_thousands'] = matches2['Population']/100000\n",
    "matches2['Pop_under_1_mil'] = np.where(matches2['Population']<1e6, 1, 0)\n",
    "matches2['Pay_Floor_Annual'] = matches2['Pay_Floor_Annual'].astype(float)\n",
    "\n",
    "#re-casting outcome / treatment variables\n",
    "matches2['Apps_per_Day_log'] = matches2['Apps_per_Day_log'].astype(float)\n",
    "matches2['Remote'] = matches2['Remote'].astype(int)\n",
    "matches2['Promoted'] = matches2['Promoted'].astype(int)\n",
    "matches2['Easy_Apply'] = matches2['Easy_Apply'].astype(int)\n",
    "matches2_copy = matches2.copy()\n",
    "matches2_copy['Days_Up'] = matches2_copy['Days_Up'].astype(float)\n",
    "matches2_copy['Software_Engineer'] = np.where(matches2_copy['Job_Title'].str.contains('Software'), 1, 0)\n",
    "matches2_copy['Analyst'] = np.where(matches2_copy['Job_Title'].str.contains('Analyst'), 1, 0)\n",
    "matches2_copy['Associate'] = np.where(matches2_copy['Job_Title'].str.contains('Associate'), 1, 0)\n",
    "matches2_copy['Senior'] = np.where(matches2_copy['Job_Title'].str.contains('Senior'), 1, 0)\n",
    "co_size_df = pd.get_dummies(matches2_copy['Company_Size']).drop(columns='10,001+ employees')\n",
    "co_size_df.columns = ['UnderFiveKEmployees', 'UnderFiveHEmployees', \n",
    "                      'UnderTenKEmployees', 'UnderOneKEmployees', 'UnderTwoHEmployees']\n",
    "co_sizes = '+'.join(list(co_size_df.columns))\n",
    "cos_todummy = list(matches2_copy['Company'].value_counts()[:20].index)\n",
    "cos_df = pd.get_dummies(matches2_copy['Company'])[cos_todummy]\n",
    "cos_df.columns = [re.sub(\"[,.-/'3&]\", '', col).replace(' ', '') for col in cos_df.columns]\n",
    "cos = '+'.join(list(cos_df.columns))\n",
    "mkts_todummy = list(matches2_copy['Market'].value_counts()[:20].index)\n",
    "mkts_df = pd.get_dummies(matches2_copy['Market'])[mkts_todummy]\n",
    "mkts_df.columns = [re.sub(\"[,.-/']\", '', col).replace(' ', '') for col in mkts_df.columns]\n",
    "mkts = '+'.join(list(mkts_df.columns))\n",
    "matches2_copy = pd.concat([matches2_copy, co_size_df, cos_df, mkts_df], axis=1)\n",
    "smf.ols('Apps_per_Day_log~Remote*Population_hundred_thousands+Easy_Apply+Promoted', matches2_copy).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2ad12",
   "metadata": {},
   "source": [
    "#### Market Size HTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea05995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering to only on-site and remote postings with information on market, industry, and company size\n",
    "df_rm_os = data.copy()[(data['Remoteness']!='Hybrid')]\n",
    "df_rm_os = df_rm_os[(df_rm_os['Market']!='Not Available')&(df_rm_os['Industry']!='Not Given')]\n",
    "#treatment indicator\n",
    "df_rm_os['Remote'] = np.where(df_rm_os['Remoteness']=='Remote', 1, 0)\n",
    "#binning posting age, informed by the above visualization\n",
    "df_rm_os['Posting_Age_Bin'] = pd.cut(df_rm_os['Days_Up'], [0, 1.0, 212.0])\n",
    "df_rm_os['Posting_Age_Bin'] = df_rm_os['Posting_Age_Bin'].astype(str)\n",
    "#binning mean salary infos -- most are 0\n",
    "df_rm_os['Pay_Info_Annual_Mean'] = (df_rm_os['Pay_Floor_Annual']+df_rm_os['Pay_Ceiling_Annual'])/2\n",
    "df_rm_os['Pay_Info_Mean_Category'] = (pd.cut(df_rm_os['Pay_Info_Annual_Mean'], \n",
    "                                               bins=[i*10000 for i in range(26)], \n",
    "                                               include_lowest=True)).astype(str)\n",
    "df_rm_os['Pay_Info_Mean_Category'] = np.where(df_rm_os['Pay_Info_Annual_Mean']>250000, 'Over $250,000', df_rm_os['Pay_Info_Mean_Category'])\n",
    "df_rm_os['Pay_Info_Mean_Category'] = np.where(df_rm_os['Pay_Ceiling_Annual']==0, 0, df_rm_os['Pay_Info_Mean_Category'])\n",
    "#binning years experience requirement info\n",
    "df_rm_os['yrs_exp_bin'] = pd.cut(df_rm_os['Required_Years_Experience'], \n",
    "                                   bins=[i*3 for i in range(4)], \n",
    "                                   include_lowest=True)\n",
    "df_rm_os['yrs_exp_bin'] = np.where(df_rm_os['Required_Years_Experience']>=10, '10 or more', df_rm_os['yrs_exp_bin'])\n",
    "#creating a list of 6 matching variables\n",
    "matching_variables1 = ['Market', 'Industry', 'Company_Size', 'Pay_Info_Mean_Category', 'yrs_exp_bin', 'Posting_Age_Bin']\n",
    "#grouping by that list AND treatment status, then ID'ing duplicates based on matching variables, we can \n",
    "#detect where in the covariate space potential matches are present (before gauging text similarity)\n",
    "matchspots1 = (df_rm_os.groupby(matching_variables1 + ['Remote'])['Job_Title'].\n",
    "              count().reset_index()).rename(columns={'Job_Title': 'Observations'})\n",
    "matchspots1 = (matchspots1[matchspots1.duplicated(subset=matching_variables1)]\n",
    "              .drop(columns=['Remote', 'Observations']))\n",
    "matches = [] #to populate with matches\n",
    "match_id = 0 #to keep track of what matched with what\n",
    "for matchspot in range(len(matchspots1)): #i.e. for each row of promising covariate values\n",
    "    #filter the data to observations in that covariate space via a one-to-many inner join\n",
    "    df = matchspots1.iloc[matchspot, :].to_frame().T\n",
    "    df = df_rm_os.join(df.set_index(matching_variables1), \n",
    "                       on=matching_variables1, \n",
    "                       how='inner', \n",
    "                       lsuffix='_l', \n",
    "                       rsuffix='_r').reset_index(drop=True)\n",
    "    #I'll cycle through each treated unit and give it a control unit\n",
    "    treated_df = df.query('Remote==1').reset_index(drop=True)\n",
    "    control_df = df.query('Remote==0').reset_index(drop=True)\n",
    "    poss_matches = np.min([len(control_df), len(treated_df)])\n",
    "    for i in range(poss_matches):\n",
    "        matches.append(treated_df.iloc[i, :].to_frame().T.assign(Match_Id=match_id))\n",
    "        matches.append(control_df.iloc[i, :].to_frame().T.assign(Match_Id=match_id))\n",
    "        match_id += 1 #increment the match identifier\n",
    "matches = pd.concat(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#casting to float\n",
    "matches['Apps_per_Day_log'] = matches['Apps_per_Day_log'].astype(float)\n",
    "matches['Days_Up'] = matches['Days_Up'].astype(float)\n",
    "#casting to numeric\n",
    "matches['Remote'] = matches['Remote'].astype(int)\n",
    "matches['Promoted'] = matches['Promoted'].astype(int)\n",
    "matches['Easy_Apply'] = matches['Easy_Apply'].astype(int)\n",
    "#software engineer column\n",
    "matches['Software_Engineer'] = np.where(matches['Job_Title'].str.contains('Software Engineer'), 1, 0)\n",
    "#merging in population data; filtering out problematic ones for now\n",
    "incorrectmkts = ['California', 'Texas', 'Florida', 'Illinois', 'Ohio', 'Georgia', 'Michigan', 'Virginia',\n",
    "                 'Washington', 'Indiana', 'Missouri', 'Minnesota', 'Kentucky', 'Delaware']\n",
    "matches = matches[~matches['Market'].isin(incorrectmkts)]\n",
    "matches = matches.merge(census_data, on='Market', how='left')\n",
    "matches = matches[matches['Population'].notna()]\n",
    "# matches['Population_hundred_thousands'] = matches['Population']/100000\n",
    "#making dummies - company sizes + renaming and joining for model specification\n",
    "#first filtering to common company sizes\n",
    "matches = matches[matches['Company_Size'].isin(list(matches['Company_Size'].value_counts().index)[:4])]\n",
    "co_size_df = pd.get_dummies(matches['Company_Size']).drop(columns='10,001+ employees')\n",
    "co_size_df.columns = ['UnderFiveKEmployees', 'UnderTenKEmployees', 'UnderOneKEmployees']\n",
    "co_sizes = '+'.join(list(co_size_df.columns))\n",
    "#making dummies - companies with at least 50 observations + renaming and joining for model specification\n",
    "cos_todummy = list(matches['Company'].value_counts()[matches['Company'].value_counts()>=50].index)\n",
    "cos_df = pd.get_dummies(matches['Company'])[cos_todummy]\n",
    "cos_df.columns = [re.sub(\"[,.-/'3&]\", '', col).replace(' ', '') for col in cos_df.columns]\n",
    "cos = '+'.join(list(cos_df.columns))\n",
    "#making dummies - markets with at least 50 observations + renaming and joining for model specification\n",
    "mkts_todummy = list(matches['Market'].value_counts()[matches['Market'].value_counts()>=50].index)\n",
    "#joining dfs and string\n",
    "matches = pd.concat([matches, co_size_df, cos_df], axis=1)\n",
    "#randomly sampling 10 markets and ordering them by population for plotting\n",
    "np.random.seed(2023)\n",
    "idx = list(np.random.choice(np.arange(len(mkts_todummy)), size=20, replace=False))\n",
    "mkts20 = [mkts_todummy[i] for i in idx]\n",
    "mkts20 = list(matches[matches['Market'].isin(mkts20)].groupby('Market')['Population'].mean().sort_values(ascending=False).index)\n",
    "#extracting HTE estimates\n",
    "Coefficients = []\n",
    "Lower_Bounds = []\n",
    "Upper_Bounds = []\n",
    "Error_Bars = []\n",
    "vs2 = co_sizes+'+'+cos\n",
    "#extract HTE estimates \n",
    "for mkt in mkts20:\n",
    "    df = matches.copy()\n",
    "    #removing stuff that causes issues in model specification\n",
    "    label = re.sub(\"[,.-/–']\", '', mkt).replace(' ', '')\n",
    "    df[f'{label}'] = np.where(df['Market']==mkt, 1, 0)\n",
    "    vs = f'Apps_per_Day_log~Remote*{label}+Easy_Apply+Promoted+Days_Up+Software_Engineer+'+vs2\n",
    "    mod = smf.ols(vs, df).fit()\n",
    "    coef = mod.params[f'Remote:{label}']\n",
    "    Coefficients.append(coef)\n",
    "    Lower_Bounds.append(coef-(1.96*mod.bse[f'Remote:{label}']))\n",
    "    Upper_Bounds.append(coef+(1.96*mod.bse[f'Remote:{label}']))\n",
    "    Error_Bars.append(1.96*mod.bse[f'Remote:{label}'])\n",
    "HTE_df = pd.DataFrame({'Market': mkts20, 'Interaction Coefficient': Coefficients, \n",
    "                       'Lower Bound': Lower_Bounds, 'Upper Bound': Upper_Bounds, \n",
    "                       'Error': Error_Bars})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12db86",
   "metadata": {},
   "source": [
    "# Hybrid/On-Site Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079eca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering to only on-site and remote postings with information on market, industry, and company size\n",
    "df_h_os = data.copy()[(data['Remoteness']!='Remote')]\n",
    "df_h_os = df_h_os[(df_h_os['Market']!='Not Available')&(df_h_os['Industry']!='Not Given')]\n",
    "#treatment indicator\n",
    "df_h_os['Hybrid'] = np.where(df_h_os['Remoteness']=='Hybrid', 1, 0)\n",
    "#binning posting age, informed by the above visualization\n",
    "df_h_os['Posting_Age_Bin'] = pd.cut(df_h_os['Days_Up'], [0, 1.0, 212.0])\n",
    "df_h_os['Posting_Age_Bin'] = df_h_os['Posting_Age_Bin'].astype(str)\n",
    "#binning mean salary infos -- most are 0\n",
    "df_h_os['Pay_Info_Annual_Mean'] = (df_h_os['Pay_Floor_Annual']+df_h_os['Pay_Ceiling_Annual'])/2\n",
    "df_h_os['Pay_Info_Mean_Category'] = (pd.cut(df_h_os['Pay_Info_Annual_Mean'], \n",
    "                                               bins=[i*10000 for i in range(26)], \n",
    "                                               include_lowest=True)).astype(str)\n",
    "df_h_os['Pay_Info_Mean_Category'] = np.where(df_h_os['Pay_Info_Annual_Mean']>250000, 'Over $250,000', df_h_os['Pay_Info_Mean_Category'])\n",
    "df_h_os['Pay_Info_Mean_Category'] = np.where(df_h_os['Pay_Ceiling_Annual']==0, 0, df_h_os['Pay_Info_Mean_Category'])\n",
    "#binning years experience requirement info\n",
    "df_h_os['yrs_exp_bin'] = pd.cut(df_h_os['Required_Years_Experience'], \n",
    "                                   bins=[i*3 for i in range(4)], \n",
    "                                   include_lowest=True)\n",
    "df_h_os['yrs_exp_bin'] = np.where(df_h_os['Required_Years_Experience']>=10, '10 or more', df_h_os['yrs_exp_bin'])\n",
    "#creating a list of 6 matching variables\n",
    "matching_variables2 = ['Market', 'Industry', 'Company_Size', 'Pay_Info_Mean_Category', 'yrs_exp_bin', 'Posting_Age_Bin']\n",
    "#grouping by that list AND treatment status, then ID'ing duplicates based on matching variables, we can \n",
    "#detect where in the covariate space potential matches are present (before gauging text similarity)\n",
    "matchspots2 = (df_h_os.groupby(matching_variables2 + ['Hybrid'])['Job_Title'].\n",
    "              count().reset_index()).rename(columns={'Job_Title': 'Observations'})\n",
    "matchspots2 = (matchspots2[matchspots2.duplicated(subset=matching_variables2)]\n",
    "              .drop(columns=['Hybrid', 'Observations']))\n",
    "\n",
    "#Now finally I'll trawl through those covariate spaces and log t/c pairings similar enough on title as matches\n",
    "matches3 = [] #to populate with matches\n",
    "match_id = 0 #to keep track of what matched with what\n",
    "\n",
    "for matchspot in range(len(matchspots2)): #i.e. for each row of promising covariate values\n",
    "    #filter the data to observations in that covariate space via a one-to-many inner join\n",
    "    df = matchspots2.iloc[matchspot, :].to_frame().T\n",
    "    df = df_h_os.join(df.set_index(matching_variables2), \n",
    "                       on=matching_variables2, \n",
    "                       how='inner', \n",
    "                       lsuffix='_l', \n",
    "                       rsuffix='_r').reset_index(drop=True)\n",
    "    #record a cutoff pt. between existing and forthcoming columns; only the latter will be used to gauge similarity\n",
    "    col_count = df.shape[1]\n",
    "    #a vector of titles, stripped of any references to treatment with the title_cleaner function\n",
    "    titles = [title_cleaner(title) for title in df['Job_Title'].copy()]\n",
    "    #initializing a model that records all unigrams as features,  fitting it on titles, and cleaning future col names\n",
    "    vect = CountVectorizer(ngram_range=(1,1))\n",
    "    vect.fit(titles)\n",
    "    feats = [feat.replace(' ', '') for feat in vect.get_feature_names()]\n",
    "    #appending each posting's new features; use of Jaccard distance to gauge similarity necessitates Boolean\n",
    "    titles_matrix = vect.transform(titles)\n",
    "    text_df = pd.DataFrame(titles_matrix.toarray().astype(bool), columns=feats).reset_index(drop=True)\n",
    "    df = pd.concat([df, text_df], axis=1)\n",
    "    #features in hand, I'll cycle through each treated unit in the covariate space and find\n",
    "    #the control neighbor nearest to it based on those features (conditional on exact-matching legwork already done)\n",
    "    treated_df = df.query('Hybrid==1').reset_index(drop=True)\n",
    "    control_df = df.query('Hybrid==0').reset_index(drop=True)\n",
    "    #the model is re-fit for every treated unit in case I want to match without replacement,\n",
    "    #though the order-dependence of that might really hurt/destabilize the matching process\n",
    "    for i in range(len(treated_df)):\n",
    "        #a control unit model; predictions with it surface control neighbors\n",
    "        #(outcome variable Apps per Day not used)\n",
    "        control_neighbor_finder = (NearestNeighbors(n_neighbors=1, metric='jaccard').\n",
    "                                   fit(np.array(control_df.iloc[:, col_count:]), control_df['Apps_per_Day'])\n",
    "                                  )\n",
    "        #the treated unit's text features as boolean values\n",
    "        tdatum = np.array(treated_df.iloc[i, col_count:].to_frame().T, dtype=bool)\n",
    "        #the treated unit's distance from its nearest neighbor and the NN itself's index value in control_df\n",
    "        dist, neigh_loc = control_neighbor_finder.kneighbors(tdatum, n_neighbors=1)\n",
    "        dist, neigh_loc = dist[0][0], neigh_loc[0][0] #extracting scalar values\n",
    "        #if it's pretty close in job title to a control title in the covariate space, record both\n",
    "        if dist < .6: #this is subjective. But I think strict enough using Jaccard distance\n",
    "            matches3.append(treated_df.iloc[i, :col_count].to_frame().T.assign(Title_Match_Id=match_id))\n",
    "            matches3.append(control_df.iloc[neigh_loc, :col_count].to_frame().T.assign(Title_Match_Id=match_id))\n",
    "            match_id += 1 #increment the match identifier\n",
    "        else:\n",
    "            pass #since the treated unit's possible match was a poor one, discard it\n",
    "#turn the list of matched observations into a dataframe\n",
    "matches3 = pd.concat(matches3)\n",
    "#finally, matching completed, it's maybe worth noting how small the unique set of control observations is\n",
    "#if way smaller, we're relying on a relatively small set of control units, increasing variance\n",
    "#a way to offset this is to increase the distance threshold (e.g. from .6 to .7), which of course will increase bias\n",
    "#another way is to match without replacement, which also increases bias because this also worsens average match quality\n",
    "t2 = matches3.query(\"Hybrid==1\")\n",
    "c2 = matches3.query(\"Hybrid==0\")\n",
    "print('The sample size is now', len(matches3))\n",
    "print(f'However, this is composed of {len(t2)} treated units')\n",
    "unique_c_count2 = c2[~c2.duplicated(subset=['Job_Title', 'Time Up', 'Date_Scraped'])].shape[0]\n",
    "print(f'and {unique_c_count2} control units.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc98709",
   "metadata": {},
   "source": [
    "#### Some More Processing+Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe35c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining market sizes according to Census data and handling duplicate market names\n",
    "matches3 = matches3.merge(census_data, on='Market', how='left')\n",
    "matches3.loc[matches3['Location'].str.contains('Portland, ME'), 'Population'] = 68313 \n",
    "matches3.loc[matches3['Location'].str.contains('Louisville, CO'), 'Population'] = 20975 \n",
    "matches3.loc[matches3['Location'].str.contains('Wyoming, MN'), 'Population'] = 7791\n",
    "matches3.loc[matches3['Location'].str.contains('Arlington, VA'), 'Population'] = 232965\n",
    "matches3.loc[matches3['Location'].str.contains('Lexington, MA'), 'Population'] = 33792\n",
    "matches3.loc[matches3['Location'].str.contains('Riverside, RI'), 'Population'] = 15912\n",
    "matches3.loc[matches3['Location'].str.contains('Newark, DE'), 'Population'] = 31155\n",
    "matches3.loc[matches3['Location'].str.contains('Rochester, MN'), 'Population'] = 121465\n",
    "matches3.loc[matches3['Location'].str.contains('Peoria, IL'), 'Population'] = 111666\n",
    "matches3.loc[matches3['Location'].str.contains('Charleston, WV'), 'Population'] = 48018\n",
    "matches3.loc[matches3['Location'].str.contains('Columbia, MO'), 'Population'] = 126853\n",
    "matches3.loc[matches3['Location'].str.contains('Columbia, MD'), 'Population'] = 105412\n",
    "matches3.loc[matches3['Location'].str.contains('Hamilton, OH'), 'Population'] = 62947\n",
    "matches3.loc[matches3['Location'].str.contains('Greenville, OH'), 'Population'] = 12715\n",
    "matches3.loc[matches3['Location'].str.contains('Ogden, IL'), 'Population'] = 721\n",
    "matches3.loc[matches3['Location'].str.contains('Auburn, NY'), 'Population'] = 26664\n",
    "matches3.loc[matches3['Location'].str.contains('Plymouth, MI'), 'Population'] = 9313\n",
    "matches3.loc[matches3['Location'].str.contains('Union City, GA'), 'Population'] = 27359\n",
    "matches3.loc[matches3['Location'].str.contains('Columbus, OH'), 'Population'] = 906528\n",
    "matches3.loc[matches3['Location'].str.contains('Columbus, NE'), 'Population'] = 24123\n",
    "matches3.loc[matches3['Location'].str.contains('Columbus, GA'), 'Population'] = 197485\n",
    "matches3.loc[matches3['Location'].str.contains('Deerfield, IL'), 'Population'] = 19079\n",
    "matches3.loc[matches3['Location'].str.contains('Wilton, CA'), 'Population'] = 5224\n",
    "matches3.loc[matches3['Location'].str.contains('Brooklyn, NY'), 'Population'] = 2577000\n",
    "matches3.loc[matches3['Location'].str.contains('Piedmont, SC'), 'Population'] = 6122\n",
    "matches3['Population_hundred_thousands'] = matches3['Population']/100000\n",
    "#re-casting outcome / treatment variables\n",
    "matches3['Apps_per_Day_log'] = matches3['Apps_per_Day_log'].astype(float)\n",
    "matches3['Hybrid'] = matches3['Hybrid'].astype(int)\n",
    "matches3['Days_Up'] = matches3['Days_Up'].astype(float)\n",
    "matches3_copy = matches3.copy()\n",
    "matches3_copy['Software_Engineer'] = np.where(matches3_copy['Job_Title'].str.contains('Software'), 1, 0)\n",
    "matches3_copy['Analyst'] = np.where(matches3_copy['Job_Title'].str.contains('Analyst'), 1, 0)\n",
    "matches3_copy['Associate'] = np.where(matches3_copy['Job_Title'].str.contains('Associate'), 1, 0)\n",
    "matches3_copy['Senior'] = np.where(matches3_copy['Job_Title'].str.contains('Senior'), 1, 0)\n",
    "co_size_df = pd.get_dummies(matches3_copy['Company_Size']).drop(columns='10,001+ employees')\n",
    "co_size_df.columns = ['UnderFiveKEmployees', 'UnderTenEmployees', 'UnderFiftyEmployees', 'UnderFiveHEmployees', \n",
    "                      'UnderTenKEmployees', 'UnderOneKEmployees', 'UnderTwoHEmployees', 'EmployeesNotGiven']\n",
    "co_sizes = '+'.join(list(co_size_df.columns))\n",
    "cos_todummy = list(matches3_copy['Company'].value_counts()[:20].index)\n",
    "cos_df = pd.get_dummies(matches3_copy['Company'])[cos_todummy]\n",
    "cos_df = cos_df.rename(columns={'Grant Thornton LLP (US)': 'GrantThorntonLLP'})\n",
    "cos_df.columns = [re.sub(\"[,.-/'3&]\", '', col).replace(' ', '') for col in cos_df.columns]\n",
    "cos = '+'.join(list(cos_df.columns))\n",
    "mkts_todummy = list(matches3_copy['Market'].value_counts()[:20].index)\n",
    "mkts_df = pd.get_dummies(matches3_copy['Market'])[mkts_todummy]\n",
    "mkts_df.columns = [re.sub(\"[,.-/']\", '', col).replace(' ', '') for col in mkts_df.columns]\n",
    "mkts = '+'.join(list(mkts_df.columns))\n",
    "matches3_copy = pd.concat([matches3_copy, co_size_df, cos_df, mkts_df], axis=1)\n",
    "smf.ols('Apps_per_Day_log~Hybrid*Population_hundred_thousands+Easy_Apply+Promoted', matches3_copy).fit().summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
